# @author: José Arbelaez
"""
benchmark_visual_reporter_multimode.py

Extended Benchmark Visual Evaluation Pack for multi-mode benchmark results.
Generates comprehensive figures, tables, and reports considering:
- CV modes: simple vs nested
- Samplers: Sobol vs LHS
- Training sizes: multiple n_train values
- GP predictions with uncertainty visualization

This module EXTENDS (does NOT replace) benchmark_visual_reporter.py.

Usage:
    python -m src.evaluation.benchmark_visual_reporter_multimode --session comprehensive_20260121_...
    
    Or programmatically:
        from src.evaluation.benchmark_visual_reporter_multimode import generate_benchmark_report_multimode
        generate_benchmark_report_multimode("comprehensive_20260121_...")

Requirements:
    The input JSON must contain data for BOTH samplers (sobol, lhs) and BOTH cv_modes (simple, tuning/nested).
    If requirements are not met, the script will abort with a clear error message.
"""

from __future__ import annotations

import ast
import json
import logging
import re
import warnings
from dataclasses import dataclass, field
from pathlib import Path
from typing import Dict, List, Optional, Tuple, Union, Any
from collections import Counter, defaultdict
from itertools import combinations, product
import traceback

# Use non-interactive backend for batch generation
import matplotlib
matplotlib.use('Agg')

import matplotlib.pyplot as plt
import matplotlib.patches as mpatches
import matplotlib.gridspec as gridspec
from matplotlib.colors import LogNorm, Normalize
import numpy as np
import pandas as pd
import seaborn as sns
from scipy import stats

# Project imports
from src.utils.paths import LOGS_DIR, PLOTS_DIR

# Import from existing module for reuse
from src.evaluation.benchmark_visual_reporter import (
    MODEL_FAMILY_PALETTE,
    GP_KERNEL_PALETTE,
    NOISE_PALETTE,
    MODEL_VARIANT_PALETTE,
    BenchmarkGlobalPlotter,
)

# =============================================================================
# PUBLICATION-READY STYLE CONFIGURATION
# =============================================================================

sns.set_theme(style="whitegrid", context="paper", font_scale=1.2)
plt.rcParams.update({
    'figure.dpi': 200,
    'savefig.dpi': 300,
    'axes.labelsize': 12,
    'axes.titlesize': 14,
    'legend.fontsize': 10,
    'xtick.labelsize': 10,
    'ytick.labelsize': 10,
    'lines.linewidth': 1.5,
    'figure.facecolor': 'white',
    'axes.facecolor': 'white',
    'savefig.facecolor': 'white',
    'font.family': 'sans-serif',
})

# Extended color palettes
SAMPLER_PALETTE = {
    "sobol": "#e74c3c",   # Red
    "lhs": "#3498db",     # Blue
}

CV_MODE_PALETTE = {
    "simple": "#2ecc71",   # Green
    "tuning": "#9b59b6",   # Purple
    "nested": "#9b59b6",   # Purple (alias)
}

NTRAIN_CMAP = plt.cm.viridis


# =============================================================================
# DATA CLASSES
# =============================================================================

@dataclass
class MultiModeRequirements:
    """Requirements verification result."""
    has_sobol: bool = False
    has_lhs: bool = False
    has_simple: bool = False
    has_tuning: bool = False
    samplers_found: List[str] = field(default_factory=list)
    cv_modes_found: List[str] = field(default_factory=list)
    n_train_values: List[int] = field(default_factory=list)
    noise_types: List[str] = field(default_factory=list)
    benchmarks: List[str] = field(default_factory=list)
    models: List[str] = field(default_factory=list)
    missing: List[str] = field(default_factory=list)
    
    @property
    def is_valid(self) -> bool:
        return self.has_sobol and self.has_lhs and self.has_simple and self.has_tuning
    
    def get_error_message(self) -> str:
        if self.is_valid:
            return ""
        missing = []
        if not self.has_sobol:
            missing.append("sampler=sobol")
        if not self.has_lhs:
            missing.append("sampler=lhs")
        if not self.has_simple:
            missing.append("cv_mode=simple")
        if not self.has_tuning:
            missing.append("cv_mode=tuning/nested")
        return f"No se puede ejecutar benchmark_visual_reporter_multimode: faltan {{{', '.join(missing)}}}"


@dataclass
class DataQualityReportMultiMode:
    """Extended data quality report for multi-mode evaluation."""
    total_records: int
    unique_configs: int
    samplers: List[str]
    cv_modes: List[str]
    n_train_values: List[int]
    benchmarks: List[str]
    noise_types: List[str]
    models: List[str]
    missing_fields: Dict[str, int]
    completeness_by_factor: Dict[str, float]
    recommendations: List[str]


@dataclass
class BenchmarkInventoryMultiMode:
    """Extended inventory for multi-mode benchmark session."""
    session_name: str
    timestamp: str
    total_time_s: float
    samplers: List[str]
    cv_modes: List[str]
    n_train_values: List[int]
    benchmarks: List[str]
    noise_types: List[str]
    models: List[str]
    n_results: int


# =============================================================================
# REQUIREMENTS VERIFIER
# =============================================================================

class MultiModeReportVerifier:
    """
    Verifies that the benchmark results contain all required factors
    for multi-mode analysis (sobol+lhs, simple+nested).
    """
    
    REQUIRED_SAMPLERS = {"sobol", "lhs"}
    REQUIRED_CV_MODES = {"simple", "tuning"}  # tuning = nested
    
    def __init__(self, results_data: Dict, strict: bool = True):
        """
        Args:
            results_data: Loaded JSON data (comprehensive_results.json)
            strict: If True, abort if requirements not met
        """
        self.data = results_data
        self.strict = strict
        self.requirements = MultiModeRequirements()
        
    def verify(self) -> MultiModeRequirements:
        """
        Verify that all required factors are present.
        
        Returns:
            MultiModeRequirements with verification results
            
        Raises:
            ValueError if strict=True and requirements not met
        """
        # Extract metadata
        metadata = self.data.get("metadata", {})
        results = self.data.get("results", {})
        
        # Check samplers
        samplers = set(metadata.get("samplers", []))
        if not samplers and results:
            samplers = set(results.keys())
        
        self.requirements.samplers_found = sorted(samplers)
        self.requirements.has_sobol = "sobol" in samplers
        self.requirements.has_lhs = "lhs" in samplers
        
        # Check cv_modes
        cv_modes = set()
        if metadata.get("cv_mode") == "both":
            cv_modes = {"simple", "tuning"}
        elif metadata.get("cv_mode"):
            cv_modes = {metadata["cv_mode"]}
        else:
            # Scan results to find cv_modes
            for sampler_data in results.values():
                if isinstance(sampler_data, dict):
                    for ntrain_data in sampler_data.values():
                        if isinstance(ntrain_data, dict):
                            for bench_data in ntrain_data.values():
                                if isinstance(bench_data, dict):
                                    for noise_data in bench_data.values():
                                        if isinstance(noise_data, dict):
                                            cv_modes.update(noise_data.keys())
        
        self.requirements.cv_modes_found = sorted(cv_modes)
        self.requirements.has_simple = "simple" in cv_modes
        self.requirements.has_tuning = "tuning" in cv_modes or "nested" in cv_modes
        
        # Extract other factors
        self.requirements.n_train_values = sorted(metadata.get("n_train_list", []))
        self.requirements.noise_types = [n.get("type", "unknown") for n in metadata.get("noise_configs", [])]
        self.requirements.benchmarks = metadata.get("benchmarks", [])
        self.requirements.models = metadata.get("models", [])
        
        # Build missing list
        if not self.requirements.has_sobol:
            self.requirements.missing.append("sampler=sobol")
        if not self.requirements.has_lhs:
            self.requirements.missing.append("sampler=lhs")
        if not self.requirements.has_simple:
            self.requirements.missing.append("cv_mode=simple")
        if not self.requirements.has_tuning:
            self.requirements.missing.append("cv_mode=tuning")
        
        # Strict mode: raise error if invalid
        if self.strict and not self.requirements.is_valid:
            raise ValueError(self.requirements.get_error_message())
        
        return self.requirements


# =============================================================================
# EXTENDED DATA LOADER
# =============================================================================

class MultiModeResultsLoader:
    """
    Loads and normalizes comprehensive benchmark results with multi-mode support.
    
    Handles the nested structure:
        results[sampler][n_train][benchmark][noise][cv_mode][model] = metrics
    """
    
    def __init__(self, results_json_path: Path, strict: bool = True):
        """
        Args:
            results_json_path: Path to comprehensive_results.json
            strict: If True, abort if requirements not met
        """
        self.json_path = Path(results_json_path)
        self.session_name = self.json_path.parent.name
        self.strict = strict
        
        self._raw_data = None
        self._master_df = None
        self._requirements = None
        
    def load_raw(self) -> Dict:
        """Load raw JSON data."""
        if self._raw_data is None:
            with open(self.json_path, 'r', encoding='utf-8') as f:
                self._raw_data = json.load(f)
        return self._raw_data
    
    def verify_requirements(self) -> MultiModeRequirements:
        """Verify multi-mode requirements."""
        if self._requirements is None:
            raw = self.load_raw()
            verifier = MultiModeReportVerifier(raw, strict=self.strict)
            self._requirements = verifier.verify()
        return self._requirements
    
    def _parse_kernel_variant(self, kernel_str: str) -> str:
        """Parse GP kernel string to variant name."""
        if not kernel_str:
            return "Unknown"
        
        kernel_lower = kernel_str.lower()
        
        if "matern" in kernel_lower:
            nu_match = re.search(r'nu\s*=\s*([\d.]+)', kernel_str)
            if nu_match:
                nu_val = float(nu_match.group(1))
                if abs(nu_val - 1.5) < 0.1:
                    return "Matern32"
                elif abs(nu_val - 2.5) < 0.1:
                    return "Matern52"
            return "Matern"
        elif "rbf" in kernel_lower:
            return "RBF"
        else:
            return "Custom"
    
    def _extract_model_variant(self, model_name: str, model_params: Dict) -> Tuple[str, str]:
        """Extract model_family and model_variant."""
        # Simplified model name extraction
        name_lower = model_name.lower()
        
        if "dummy" in name_lower:
            return "Dummy", "Dummy"
        elif "ridge" in name_lower:
            return "Ridge", "Ridge"
        elif "pls" in name_lower:
            n_comp = model_params.get("n_components", "?")
            return "PLS", f"PLS_n{n_comp}"
        elif "gp" in name_lower:
            kernel_str = str(model_params.get("kernel", ""))
            kernel_variant = self._parse_kernel_variant(kernel_str)
            return "GP", f"GP_{kernel_variant}"
        else:
            return model_name, model_name
    
    def build_master_table(self) -> pd.DataFrame:
        """
        Build normalized master table from nested results structure.
        
        Returns:
            DataFrame with columns:
                sampler, n_train, benchmark, noise, cv_mode, model, model_family, model_variant,
                mae, rmse, r2, nlpd, coverage_95, calibration_error_95, sharpness, fit_time, ...
        """
        if self._master_df is not None:
            return self._master_df
        
        # Verify requirements first
        self.verify_requirements()
        
        raw = self.load_raw()
        metadata = raw.get("metadata", {})
        results = raw.get("results", {})
        
        records = []
        
        # Navigate nested structure
        for sampler, sampler_data in results.items():
            if not isinstance(sampler_data, dict):
                continue
                
            for n_train_str, ntrain_data in sampler_data.items():
                if not isinstance(ntrain_data, dict):
                    continue
                try:
                    n_train = int(n_train_str)
                except (ValueError, TypeError):
                    continue
                
                for benchmark, bench_data in ntrain_data.items():
                    if not isinstance(bench_data, dict):
                        continue
                    
                    for noise, noise_data in bench_data.items():
                        if not isinstance(noise_data, dict):
                            continue
                        
                        for cv_mode, cv_data in noise_data.items():
                            if not isinstance(cv_data, dict):
                                continue
                            
                            for model_name, model_results in cv_data.items():
                                if not isinstance(model_results, dict):
                                    continue
                                
                                # Skip error entries
                                if "error" in model_results:
                                    continue
                                
                                # Extract model params
                                model_params = model_results.get("model_params", {})
                                if isinstance(model_params, str):
                                    try:
                                        model_params = json.loads(model_params)
                                    except:
                                        model_params = {}
                                
                                family, variant = self._extract_model_variant(model_name, model_params)
                                
                                record = {
                                    "sampler": sampler,
                                    "n_train": n_train,
                                    "benchmark": benchmark,
                                    "noise": noise,
                                    "cv_mode": cv_mode,
                                    "model": model_name,
                                    "model_family": family,
                                    "model_variant": variant,
                                    # Metrics
                                    "mae": model_results.get("mae"),
                                    "rmse": model_results.get("rmse"),
                                    "r2": model_results.get("r2"),
                                    "nlpd": model_results.get("nlpd"),
                                    "coverage_50": model_results.get("coverage_50"),
                                    "coverage_90": model_results.get("coverage_90"),
                                    "coverage_95": model_results.get("coverage_95"),
                                    "calibration_error_95": model_results.get("calibration_error_95"),
                                    "sharpness": model_results.get("sharpness"),
                                    "fit_time": model_results.get("fit_time"),
                                    "predict_time": model_results.get("predict_time"),
                                    # For tuning mode
                                    "mae_std": model_results.get("mae_std"),
                                    "rmse_std": model_results.get("rmse_std"),
                                    "r2_std": model_results.get("r2_std"),
                                    "n_folds": model_results.get("n_folds"),
                                }
                                
                                records.append(record)
        
        self._master_df = pd.DataFrame(records)
        
        # Ensure proper column order
        primary_cols = [
            "sampler", "n_train", "benchmark", "noise", "cv_mode",
            "model", "model_family", "model_variant",
            "mae", "rmse", "r2", "nlpd", "coverage_95",
            "calibration_error_95", "sharpness", "fit_time"
        ]
        other_cols = [c for c in self._master_df.columns if c not in primary_cols]
        ordered_cols = [c for c in primary_cols if c in self._master_df.columns] + other_cols
        self._master_df = self._master_df[ordered_cols]
        
        logging.info(f"Built master table with {len(self._master_df)} records")
        return self._master_df
    
    def get_inventory(self) -> BenchmarkInventoryMultiMode:
        """Get inventory of the multi-mode benchmark session."""
        raw = self.load_raw()
        df = self.build_master_table()
        metadata = raw.get("metadata", {})
        
        return BenchmarkInventoryMultiMode(
            session_name=self.session_name,
            timestamp=metadata.get("timestamp", ""),
            total_time_s=metadata.get("total_time_s", 0),
            samplers=sorted(df["sampler"].unique()),
            cv_modes=sorted(df["cv_mode"].unique()),
            n_train_values=sorted(df["n_train"].unique()),
            benchmarks=sorted(df["benchmark"].unique()),
            noise_types=sorted(df["noise"].unique()),
            models=sorted(df["model_variant"].unique()),
            n_results=len(df),
        )
    
    def export_master_table(self, output_dir: Path) -> Path:
        """Export master table to CSV."""
        output_dir = Path(output_dir)
        output_dir.mkdir(parents=True, exist_ok=True)
        
        path = output_dir / "master_table.csv"
        self.build_master_table().to_csv(path, index=False)
        logging.info(f"Exported master table: {path}")
        return path


# =============================================================================
# EXTENDED TABLE GENERATOR
# =============================================================================

class MultiModeTableGenerator:
    """
    Generates tables for multi-mode benchmark analysis.
    """
    
    def __init__(self, master_df: pd.DataFrame, output_dir: Path):
        self.df = master_df.copy()
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(parents=True, exist_ok=True)
    
    def generate_leaderboards_disaggregated(self, 
                                            metrics: List[str] = None,
                                            top_n: int = 20) -> Dict[str, pd.DataFrame]:
        """
        Generate leaderboards disaggregated by sampler, cv_mode, n_train.
        """
        if metrics is None:
            metrics = ["mae", "rmse", "r2"]
        
        leaderboards = {}
        
        for metric in metrics:
            if metric not in self.df.columns:
                continue
            
            ascending = metric != "r2"
            
            # Global leaderboard (aggregated)
            global_lb = (self.df.groupby("model_variant")[metric]
                         .agg(["mean", "std", "count"])
                         .reset_index()
                         .sort_values("mean", ascending=ascending)
                         .head(top_n))
            global_lb["rank"] = range(1, len(global_lb) + 1)
            global_lb.to_csv(self.output_dir / f"leaderboard_global_{metric}.csv", index=False)
            leaderboards[f"global_{metric}"] = global_lb
            
            # By sampler
            for sampler in self.df["sampler"].unique():
                sampler_df = self.df[self.df["sampler"] == sampler]
                lb = (sampler_df.groupby("model_variant")[metric]
                      .agg(["mean", "std", "count"])
                      .reset_index()
                      .sort_values("mean", ascending=ascending)
                      .head(top_n))
                lb["rank"] = range(1, len(lb) + 1)
                lb.to_csv(self.output_dir / f"leaderboard_{sampler}_{metric}.csv", index=False)
                leaderboards[f"{sampler}_{metric}"] = lb
            
            # By cv_mode
            for cv_mode in self.df["cv_mode"].unique():
                cv_df = self.df[self.df["cv_mode"] == cv_mode]
                lb = (cv_df.groupby("model_variant")[metric]
                      .agg(["mean", "std", "count"])
                      .reset_index()
                      .sort_values("mean", ascending=ascending)
                      .head(top_n))
                lb["rank"] = range(1, len(lb) + 1)
                lb.to_csv(self.output_dir / f"leaderboard_{cv_mode}_{metric}.csv", index=False)
                leaderboards[f"{cv_mode}_{metric}"] = lb
            
            # By n_train
            for n_train in sorted(self.df["n_train"].unique()):
                ntrain_df = self.df[self.df["n_train"] == n_train]
                lb = (ntrain_df.groupby("model_variant")[metric]
                      .agg(["mean", "std", "count"])
                      .reset_index()
                      .sort_values("mean", ascending=ascending)
                      .head(top_n))
                lb["rank"] = range(1, len(lb) + 1)
                lb.to_csv(self.output_dir / f"leaderboard_ntrain{n_train}_{metric}.csv", index=False)
                leaderboards[f"ntrain{n_train}_{metric}"] = lb
        
        logging.info(f"Generated {len(leaderboards)} disaggregated leaderboards")
        return leaderboards
    
    def generate_stability_tables(self) -> Dict[str, pd.DataFrame]:
        """
        Generate stability analysis tables (std, IQR, CV).
        """
        tables = {}
        
        for metric in ["mae", "rmse", "r2"]:
            if metric not in self.df.columns:
                continue
            
            # Group by model_variant and compute stability metrics
            stability = (self.df.groupby("model_variant")[metric]
                         .agg([
                             "mean", "std", "median",
                             lambda x: x.quantile(0.25),
                             lambda x: x.quantile(0.75),
                             lambda x: x.std() / x.mean() if x.mean() != 0 else np.nan  # CV
                         ])
                         .reset_index())
            stability.columns = ["model_variant", "mean", "std", "median", "p25", "p75", "cv"]
            stability["iqr"] = stability["p75"] - stability["p25"]
            stability = stability.sort_values("cv", ascending=True)
            
            path = self.output_dir / f"stability_{metric}.csv"
            stability.to_csv(path, index=False)
            tables[metric] = stability
        
        # Stability by group (sampler, cv_mode, n_train)
        group_stability = (self.df.groupby(["sampler", "cv_mode", "n_train", "model_variant"])["mae"]
                          .agg(["mean", "std"])
                          .reset_index())
        group_stability.to_csv(self.output_dir / "stability_by_group.csv", index=False)
        tables["by_group"] = group_stability
        
        logging.info(f"Generated {len(tables)} stability tables")
        return tables
    
    def generate_topx_tables(self, x: int = 10, metric: str = "mae") -> Dict[str, pd.DataFrame]:
        """
        Generate Top-X tables for each benchmark.
        """
        ascending = metric != "r2"
        tables = {}
        
        topx_dir = self.output_dir / "topX_by_problem"
        topx_dir.mkdir(exist_ok=True)
        
        for benchmark in self.df["benchmark"].unique():
            bench_df = self.df[self.df["benchmark"] == benchmark]
            
            # For each combination of factors
            for sampler in bench_df["sampler"].unique():
                for cv_mode in bench_df["cv_mode"].unique():
                    for noise in bench_df["noise"].unique():
                        subset = bench_df[
                            (bench_df["sampler"] == sampler) &
                            (bench_df["cv_mode"] == cv_mode) &
                            (bench_df["noise"] == noise)
                        ]
                        
                        if subset.empty:
                            continue
                        
                        # Aggregate by model_variant (across n_train)
                        topx = (subset.groupby("model_variant")[metric]
                                .agg(["mean", "std", "count"])
                                .reset_index()
                                .sort_values("mean", ascending=ascending)
                                .head(x))
                        topx["rank"] = range(1, len(topx) + 1)
                        
                        filename = f"top{x}_{benchmark}_{metric}_{sampler}_{cv_mode}_{noise}.csv"
                        topx.to_csv(topx_dir / filename, index=False)
                        tables[f"{benchmark}_{sampler}_{cv_mode}_{noise}"] = topx
        
        logging.info(f"Generated {len(tables)} Top-{x} tables")
        return tables
    
    def generate_best_by_factor(self, metric: str = "mae") -> pd.DataFrame:
        """
        Generate table of best model per (benchmark, sampler, cv_mode, n_train).
        """
        ascending = metric != "r2"
        
        results = []
        
        for (bench, sampler, cv_mode, n_train), group in self.df.groupby(
            ["benchmark", "sampler", "cv_mode", "n_train"]
        ):
            if group[metric].isna().all():
                continue
            
            sorted_group = group.sort_values(metric, ascending=ascending)
            best = sorted_group.iloc[0]
            
            results.append({
                "benchmark": bench,
                "sampler": sampler,
                "cv_mode": cv_mode,
                "n_train": n_train,
                "best_model": best["model_variant"],
                f"best_{metric}": best[metric],
            })
        
        df_best = pd.DataFrame(results)
        df_best.to_csv(self.output_dir / f"best_model_per_benchmark_by_sampler_cv_ntrain_{metric}.csv", index=False)
        
        return df_best
    
    def generate_all_tables(self) -> Dict[str, Any]:
        """Generate all multi-mode tables."""
        logging.info("Generating all multi-mode tables...")
        
        results = {
            "leaderboards": self.generate_leaderboards_disaggregated(),
            "stability": self.generate_stability_tables(),
            "topx_mae": self.generate_topx_tables(x=10, metric="mae"),
            "topx_rmse": self.generate_topx_tables(x=10, metric="rmse"),
            "best_by_factor_mae": self.generate_best_by_factor("mae"),
            "best_by_factor_rmse": self.generate_best_by_factor("rmse"),
        }
        
        logging.info(f"All tables saved to: {self.output_dir}")
        return results


# =============================================================================
# SAMPLING EFFECTS PLOTTER
# =============================================================================

class SamplingEffectsPlotter:
    """
    Generates plots showing effects of sampling strategy and n_train.
    """
    
    def __init__(self, master_df: pd.DataFrame, output_dir: Path):
        self.df = master_df.copy()
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(parents=True, exist_ok=True)
    
    def _get_model_color(self, model_variant: str) -> str:
        """Get consistent color for model variant."""
        if model_variant in MODEL_VARIANT_PALETTE:
            return MODEL_VARIANT_PALETTE[model_variant]
        if model_variant.startswith("GP"):
            return "#2ecc71"
        elif model_variant.startswith("PLS"):
            return "#9b59b6"
        elif model_variant.startswith("Ridge"):
            return "#3498db"
        return "#95a5a6"
    
    def plot_metric_vs_ntrain(self, metric: str = "mae", 
                              benchmark: str = None,
                              save: bool = True) -> plt.Figure:
        """
        Plot metric vs n_train (learning curves) for each model.
        """
        if benchmark:
            df = self.df[self.df["benchmark"] == benchmark]
            title_suffix = f" - {benchmark}"
        else:
            df = self.df
            title_suffix = " (Global Average)"
        
        if df.empty:
            return None
        
        # Average by (n_train, model_variant, sampler)
        agg = (df.groupby(["n_train", "model_variant", "sampler"])[metric]
               .agg(["mean", "std"])
               .reset_index())
        
        n_samplers = agg["sampler"].nunique()
        fig, axes = plt.subplots(1, n_samplers, figsize=(7 * n_samplers, 6), sharey=True)
        
        if n_samplers == 1:
            axes = [axes]
        
        for ax, sampler in zip(axes, sorted(agg["sampler"].unique())):
            sampler_data = agg[agg["sampler"] == sampler]
            
            for model in sorted(sampler_data["model_variant"].unique()):
                model_data = sampler_data[sampler_data["model_variant"] == model]
                model_data = model_data.sort_values("n_train")
                
                color = self._get_model_color(model)
                ax.plot(model_data["n_train"], model_data["mean"], 
                        'o-', color=color, label=model, linewidth=2, markersize=6)
                
                if model_data["std"].notna().any():
                    ax.fill_between(
                        model_data["n_train"],
                        model_data["mean"] - model_data["std"],
                        model_data["mean"] + model_data["std"],
                        color=color, alpha=0.2
                    )
            
            ax.set_xlabel("n_train", fontsize=12)
            ax.set_ylabel(metric.upper(), fontsize=12)
            ax.set_title(f"Sampler: {sampler.upper()}", fontsize=14, fontweight='bold')
            ax.legend(loc='best', fontsize=9)
            ax.grid(True, alpha=0.3)
        
        fig.suptitle(f"{metric.upper()} vs Training Size{title_suffix}", 
                     fontsize=16, fontweight='bold', y=1.02)
        plt.tight_layout()
        
        if save:
            suffix = f"_{benchmark}" if benchmark else "_global"
            path = self.output_dir / f"metric_vs_ntrain_{metric}{suffix}.png"
            fig.savefig(path, bbox_inches='tight', dpi=150)
            # Skip PDF to avoid slowdown
            plt.close(fig)
        
        return fig
    
    def plot_sobol_vs_lhs_comparison(self, metric: str = "mae",
                                      save: bool = True) -> plt.Figure:
        """
        Side-by-side comparison of Sobol vs LHS performance.
        """
        # Average by (sampler, model_variant)
        agg = (self.df.groupby(["sampler", "model_variant"])[metric]
               .agg(["mean", "std"])
               .reset_index())
        
        # Pivot for comparison
        pivot = agg.pivot(index="model_variant", columns="sampler", values="mean")
        
        if pivot.empty or "sobol" not in pivot.columns or "lhs" not in pivot.columns:
            logging.warning("Cannot create Sobol vs LHS comparison - missing data")
            return None
        
        fig, axes = plt.subplots(1, 2, figsize=(14, 6))
        
        # Left: Scatter plot (sobol vs lhs)
        ax = axes[0]
        colors = [self._get_model_color(m) for m in pivot.index]
        ax.scatter(pivot["sobol"], pivot["lhs"], c=colors, s=100, alpha=0.8, edgecolors='black')
        
        # Add diagonal (equal performance)
        lims = [min(pivot["sobol"].min(), pivot["lhs"].min()),
                max(pivot["sobol"].max(), pivot["lhs"].max())]
        ax.plot(lims, lims, 'k--', alpha=0.5, label='Equal performance')
        
        # Annotate points
        for i, model in enumerate(pivot.index):
            ax.annotate(model, (pivot.loc[model, "sobol"], pivot.loc[model, "lhs"]),
                        fontsize=8, xytext=(5, 5), textcoords='offset points')
        
        ax.set_xlabel(f"Sobol {metric.upper()}", fontsize=12)
        ax.set_ylabel(f"LHS {metric.upper()}", fontsize=12)
        ax.set_title("Sobol vs LHS Performance", fontsize=14, fontweight='bold')
        ax.legend()
        
        # Right: Bar plot showing difference
        ax = axes[1]
        diff = pivot["lhs"] - pivot["sobol"]  # Positive = Sobol better
        diff = diff.sort_values()
        
        colors_bar = ['#2ecc71' if d > 0 else '#e74c3c' for d in diff.values]
        ax.barh(range(len(diff)), diff.values, color=colors_bar, edgecolor='black')
        ax.set_yticks(range(len(diff)))
        ax.set_yticklabels(diff.index)
        ax.axvline(x=0, color='black', linestyle='-', linewidth=1)
        ax.set_xlabel(f"LHS - Sobol ({metric.upper()})", fontsize=12)
        ax.set_title("Performance Difference\n(Positive = Sobol better)", fontsize=14, fontweight='bold')
        
        plt.tight_layout()
        
        if save:
            path = self.output_dir / f"sobol_vs_lhs_{metric}.png"
            fig.savefig(path, bbox_inches='tight', dpi=150)
            fig.savefig(path.with_suffix('.pdf'), bbox_inches='tight')
            plt.close(fig)
        
        return fig
    
    def plot_ntrain_effect_heatmap(self, metric: str = "mae",
                                    save: bool = True) -> plt.Figure:
        """
        Heatmap showing metric by (n_train, model_variant).
        """
        # Average by (n_train, model_variant)
        pivot = (self.df.groupby(["n_train", "model_variant"])[metric]
                 .mean()
                 .unstack())
        
        if pivot.empty:
            return None
        
        fig, ax = plt.subplots(figsize=(14, 8))
        
        cmap = "RdYlGn" if metric == "r2" else "RdYlGn_r"
        sns.heatmap(pivot, annot=True, fmt=".3g", cmap=cmap,
                    ax=ax, linewidths=0.5, cbar_kws={"label": metric.upper()})
        
        ax.set_xlabel("Model Variant", fontsize=12)
        ax.set_ylabel("n_train", fontsize=12)
        ax.set_title(f"Effect of Training Size on {metric.upper()}", 
                     fontsize=14, fontweight='bold')
        
        plt.tight_layout()
        
        if save:
            path = self.output_dir / f"ntrain_effect_heatmap_{metric}.png"
            fig.savefig(path, bbox_inches='tight', dpi=150)
            plt.close(fig)
        
        return fig
    
    def plot_learning_curves_by_benchmark(self, metric: str = "mae",
                                           save: bool = True) -> Dict[str, plt.Figure]:
        """
        Generate learning curves for each benchmark (small multiples).
        """
        benchmarks = sorted(self.df["benchmark"].unique())
        figures = {}
        
        for benchmark in benchmarks:
            fig = self.plot_metric_vs_ntrain(metric=metric, benchmark=benchmark, save=save)
            if fig:
                figures[benchmark] = fig
        
        return figures
    
    def generate_all_plots(self) -> Dict[str, Any]:
        """Generate all sampling effects plots."""
        logging.info("Generating sampling effects plots...")
        
        results = {}
        
        # Metric vs n_train (global)
        for metric in ["mae", "rmse", "r2"]:
            results[f"vs_ntrain_{metric}"] = self.plot_metric_vs_ntrain(metric)
        
        # Sobol vs LHS comparison
        for metric in ["mae", "rmse"]:
            results[f"sobol_vs_lhs_{metric}"] = self.plot_sobol_vs_lhs_comparison(metric)
        
        # n_train effect heatmap
        results["ntrain_heatmap_mae"] = self.plot_ntrain_effect_heatmap("mae")
        results["ntrain_heatmap_rmse"] = self.plot_ntrain_effect_heatmap("rmse")
        
        # Learning curves by benchmark
        results["learning_curves_mae"] = self.plot_learning_curves_by_benchmark("mae")
        
        plt.close('all')
        logging.info(f"Sampling effects plots saved to: {self.output_dir}")
        return results


# =============================================================================
# CV DIAGNOSTICS PLOTTER
# =============================================================================

class CVDiagnosticsPlotter:
    """
    Generates plots comparing simple vs nested CV.
    """
    
    def __init__(self, master_df: pd.DataFrame, output_dir: Path):
        self.df = master_df.copy()
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(parents=True, exist_ok=True)
    
    def _get_model_color(self, model_variant: str) -> str:
        """Get consistent color for model variant."""
        if model_variant in MODEL_VARIANT_PALETTE:
            return MODEL_VARIANT_PALETTE[model_variant]
        return "#7f8c8d"
    
    def plot_simple_vs_nested_distribution(self, metric: str = "mae",
                                            benchmark: str = None,
                                            save: bool = True) -> plt.Figure:
        """
        Distribution comparison: simple vs nested CV.
        """
        df = self.df.copy()
        if benchmark:
            df = df[df["benchmark"] == benchmark]
            title_suffix = f" - {benchmark}"
        else:
            title_suffix = " (All Benchmarks)"
        
        if df.empty:
            return None
        
        # Check we have both cv_modes
        cv_modes = df["cv_mode"].unique()
        if len(cv_modes) < 2:
            logging.warning(f"Only one CV mode found: {cv_modes}")
            return None
        
        fig, axes = plt.subplots(1, 2, figsize=(14, 6))
        
        # Left: Violin/box plot by cv_mode
        ax = axes[0]
        models = sorted(df["model_variant"].unique())
        positions = np.arange(len(models))
        width = 0.35
        
        for i, cv_mode in enumerate(["simple", "tuning"]):
            cv_df = df[df["cv_mode"] == cv_mode]
            if cv_df.empty:
                continue
            
            data = [cv_df[cv_df["model_variant"] == m][metric].values for m in models]
            bp = ax.boxplot(data, positions=positions + (i - 0.5) * width,
                           widths=width * 0.8, patch_artist=True)
            
            color = CV_MODE_PALETTE.get(cv_mode, "#7f8c8d")
            for box in bp['boxes']:
                box.set_facecolor(color)
                box.set_alpha(0.7)
        
        ax.set_xticks(positions)
        ax.set_xticklabels(models, rotation=45, ha='right')
        ax.set_ylabel(metric.upper())
        ax.set_title(f"{metric.upper()} Distribution by CV Mode{title_suffix}", fontweight='bold')
        
        # Legend
        handles = [mpatches.Patch(color=CV_MODE_PALETTE.get(cv, "#7f8c8d"), label=cv)
                   for cv in ["simple", "tuning"]]
        ax.legend(handles=handles, loc='best')
        
        # Right: Gap plot (nested - simple)
        ax = axes[1]
        
        # Compute gap
        gaps = []
        for model in models:
            simple_val = df[(df["cv_mode"] == "simple") & (df["model_variant"] == model)][metric].mean()
            tuning_val = df[(df["cv_mode"] == "tuning") & (df["model_variant"] == model)][metric].mean()
            if pd.notna(simple_val) and pd.notna(tuning_val):
                gaps.append({
                    "model": model,
                    "gap": tuning_val - simple_val,
                    "simple": simple_val,
                    "tuning": tuning_val,
                })
        
        if gaps:
            gaps_df = pd.DataFrame(gaps).sort_values("gap")
            colors_bar = ['#2ecc71' if g < 0 else '#e74c3c' for g in gaps_df["gap"]]
            ax.barh(range(len(gaps_df)), gaps_df["gap"].values, color=colors_bar, edgecolor='black')
            ax.set_yticks(range(len(gaps_df)))
            ax.set_yticklabels(gaps_df["model"])
            ax.axvline(x=0, color='black', linestyle='-', linewidth=1)
            ax.set_xlabel(f"Tuning - Simple ({metric.upper()})")
            ax.set_title(f"CV Mode Gap\n(Negative = Tuning better)", fontweight='bold')
        
        plt.tight_layout()
        
        if save:
            suffix = f"_{benchmark}" if benchmark else "_global"
            path = self.output_dir / f"simple_vs_nested_{metric}{suffix}.png"
            fig.savefig(path, bbox_inches='tight', dpi=150)
            fig.savefig(path.with_suffix('.pdf'), bbox_inches='tight')
            plt.close(fig)
        
        return fig
    
    def plot_cv_stability_comparison(self, metric: str = "mae",
                                      save: bool = True) -> plt.Figure:
        """
        Compare stability (std) between CV modes.
        """
        # Only consider tuning mode (which has std from folds)
        tuning_df = self.df[self.df["cv_mode"] == "tuning"]
        
        if tuning_df.empty or f"{metric}_std" not in tuning_df.columns:
            logging.warning("No tuning data with std available")
            return None
        
        # Use the std from nested CV folds
        std_col = f"{metric}_std"
        
        agg = (tuning_df.groupby("model_variant")
               .agg({metric: "mean", std_col: "mean"})
               .reset_index())
        
        fig, ax = plt.subplots(figsize=(10, 6))
        
        colors = [self._get_model_color(m) for m in agg["model_variant"]]
        ax.scatter(agg[metric], agg[std_col], c=colors, s=150, alpha=0.8, edgecolors='black')
        
        for i, row in agg.iterrows():
            ax.annotate(row["model_variant"], (row[metric], row[std_col]),
                        fontsize=9, xytext=(5, 5), textcoords='offset points')
        
        ax.set_xlabel(f"Mean {metric.upper()}", fontsize=12)
        ax.set_ylabel(f"Std {metric.upper()} (across folds)", fontsize=12)
        ax.set_title("Performance vs Stability (Nested CV)", fontsize=14, fontweight='bold')
        
        plt.tight_layout()
        
        if save:
            path = self.output_dir / f"cv_stability_{metric}.png"
            fig.savefig(path, bbox_inches='tight', dpi=150)
            plt.close(fig)
        
        return fig
    
    def generate_all_plots(self) -> Dict[str, Any]:
        """Generate all CV diagnostics plots."""
        logging.info("Generating CV diagnostics plots...")
        
        results = {}
        
        # Simple vs nested distribution
        for metric in ["mae", "rmse"]:
            results[f"simple_vs_nested_{metric}"] = self.plot_simple_vs_nested_distribution(metric)
        
        # By benchmark
        for benchmark in self.df["benchmark"].unique():
            results[f"simple_vs_nested_mae_{benchmark}"] = self.plot_simple_vs_nested_distribution(
                "mae", benchmark=benchmark
            )
        
        # Stability comparison
        results["cv_stability_mae"] = self.plot_cv_stability_comparison("mae")
        
        plt.close('all')
        logging.info(f"CV diagnostics plots saved to: {self.output_dir}")
        return results


# =============================================================================
# TOP-X COMPARATOR
# =============================================================================

class TopXComparator:
    """
    Generates Top-X comparisons and cross-problem analysis.
    """
    
    def __init__(self, master_df: pd.DataFrame, output_dir: Path, top_x: int = 10):
        self.df = master_df.copy()
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(parents=True, exist_ok=True)
        self.top_x = top_x
    
    def _get_model_color(self, model_variant: str) -> str:
        """Get consistent color for model variant."""
        if model_variant in MODEL_VARIANT_PALETTE:
            return MODEL_VARIANT_PALETTE[model_variant]
        return "#7f8c8d"
    
    def plot_topx_bars(self, benchmark: str, metric: str = "mae",
                        sampler: str = None, cv_mode: str = None,
                        save: bool = True) -> plt.Figure:
        """
        Bar plot of Top-X models for a benchmark.
        """
        ascending = metric != "r2"
        
        df = self.df[self.df["benchmark"] == benchmark]
        
        if sampler:
            df = df[df["sampler"] == sampler]
        if cv_mode:
            df = df[df["cv_mode"] == cv_mode]
        
        if df.empty:
            return None
        
        # Aggregate
        agg = (df.groupby("model_variant")[metric]
               .agg(["mean", "std", "count"])
               .reset_index()
               .sort_values("mean", ascending=ascending)
               .head(self.top_x))
        
        fig, ax = plt.subplots(figsize=(12, 6))
        
        colors = [self._get_model_color(m) for m in agg["model_variant"]]
        bars = ax.bar(range(len(agg)), agg["mean"], color=colors, edgecolor='black', alpha=0.8)
        
        # Error bars
        if agg["std"].notna().any():
            ax.errorbar(range(len(agg)), agg["mean"], yerr=agg["std"],
                        fmt='none', color='black', capsize=4)
        
        ax.set_xticks(range(len(agg)))
        ax.set_xticklabels(agg["model_variant"], rotation=45, ha='right')
        ax.set_ylabel(metric.upper())
        
        # Title with all factors
        title_parts = [f"Top-{self.top_x} by {metric.upper()}", benchmark]
        if sampler:
            title_parts.append(f"sampler={sampler}")
        if cv_mode:
            title_parts.append(f"cv={cv_mode}")
        ax.set_title(" | ".join(title_parts), fontsize=14, fontweight='bold')
        
        plt.tight_layout()
        
        if save:
            suffix = f"_{sampler or 'all'}_{cv_mode or 'all'}"
            path = self.output_dir / f"top{self.top_x}_{benchmark}_{metric}{suffix}.png"
            fig.savefig(path, bbox_inches='tight', dpi=150)
            plt.close(fig)
        
        return fig
    
    def plot_cross_problem_top1_heatmap(self, metric: str = "mae",
                                         sampler: str = None,
                                         cv_mode: str = None,
                                         save: bool = True) -> plt.Figure:
        """
        Heatmap showing best model per benchmark.
        """
        ascending = metric != "r2"
        
        df = self.df.copy()
        if sampler:
            df = df[df["sampler"] == sampler]
        if cv_mode:
            df = df[df["cv_mode"] == cv_mode]
        
        if df.empty:
            return None
        
        # Find best model per benchmark
        best_models = []
        for benchmark in df["benchmark"].unique():
            bench_df = df[df["benchmark"] == benchmark]
            avg = bench_df.groupby("model_variant")[metric].mean()
            best = avg.idxmin() if ascending else avg.idxmax()
            best_models.append({"benchmark": benchmark, "best_model": best, "score": avg[best]})
        
        best_df = pd.DataFrame(best_models)
        
        # Create pivot for heatmap
        models = sorted(df["model_variant"].unique())
        benchmarks = sorted(df["benchmark"].unique())
        
        # Create matrix where 1 = best model for that benchmark
        matrix = np.zeros((len(benchmarks), len(models)))
        for i, bench in enumerate(benchmarks):
            for j, model in enumerate(models):
                if best_df[best_df["benchmark"] == bench]["best_model"].values[0] == model:
                    matrix[i, j] = 1
        
        fig, ax = plt.subplots(figsize=(14, 8))
        
        sns.heatmap(matrix, annot=False, cmap="Greens",
                    xticklabels=models, yticklabels=benchmarks,
                    ax=ax, linewidths=0.5, cbar=False)
        
        # Mark best with star
        for i, bench in enumerate(benchmarks):
            best = best_df[best_df["benchmark"] == bench]["best_model"].values[0]
            j = models.index(best)
            ax.text(j + 0.5, i + 0.5, "★", ha='center', va='center', fontsize=16, color='gold')
        
        ax.set_xlabel("Model", fontsize=12)
        ax.set_ylabel("Benchmark", fontsize=12)
        
        title_parts = [f"Best Model per Benchmark ({metric.upper()})"]
        if sampler:
            title_parts.append(f"sampler={sampler}")
        if cv_mode:
            title_parts.append(f"cv={cv_mode}")
        ax.set_title(" | ".join(title_parts), fontsize=14, fontweight='bold')
        
        plt.tight_layout()
        
        if save:
            suffix = f"_{sampler or 'all'}_{cv_mode or 'all'}"
            path = self.output_dir / f"cross_problem_top1_{metric}{suffix}.png"
            fig.savefig(path, bbox_inches='tight', dpi=150)
            fig.savefig(path.with_suffix('.pdf'), bbox_inches='tight')
            plt.close(fig)
        
        return fig
    
    def plot_topx_stability(self, benchmark: str, metric: str = "mae",
                            save: bool = True) -> plt.Figure:
        """
        Violin/box plot showing stability of Top-X models.
        """
        ascending = metric != "r2"
        
        df = self.df[self.df["benchmark"] == benchmark]
        
        if df.empty:
            return None
        
        # Get top-X models
        avg = df.groupby("model_variant")[metric].mean()
        top_models = avg.sort_values(ascending=ascending).head(self.top_x).index.tolist()
        
        top_df = df[df["model_variant"].isin(top_models)]
        
        fig, ax = plt.subplots(figsize=(12, 6))
        
        # Order by mean
        order = top_df.groupby("model_variant")[metric].mean().sort_values(ascending=ascending).index.tolist()
        
        colors = [self._get_model_color(m) for m in order]
        
        # Violin plot
        parts = ax.violinplot(
            [top_df[top_df["model_variant"] == m][metric].values for m in order],
            positions=range(len(order)),
            showmeans=False, showmedians=False, showextrema=False
        )
        
        for i, pc in enumerate(parts['bodies']):
            pc.set_facecolor(colors[i])
            pc.set_alpha(0.6)
        
        # Overlay box
        bp = ax.boxplot(
            [top_df[top_df["model_variant"] == m][metric].values for m in order],
            positions=range(len(order)),
            widths=0.3, patch_artist=True, showfliers=True
        )
        
        for i, box in enumerate(bp['boxes']):
            box.set_facecolor(colors[i])
            box.set_alpha(0.8)
        
        ax.set_xticks(range(len(order)))
        ax.set_xticklabels(order, rotation=45, ha='right')
        ax.set_ylabel(metric.upper())
        ax.set_title(f"Top-{self.top_x} Stability - {benchmark}", fontsize=14, fontweight='bold')
        
        plt.tight_layout()
        
        if save:
            path = self.output_dir / f"top{self.top_x}_stability_{benchmark}_{metric}.png"
            fig.savefig(path, bbox_inches='tight', dpi=150)
            plt.close(fig)
        
        return fig
    
    def generate_all_plots(self) -> Dict[str, Any]:
        """Generate all Top-X comparison plots."""
        logging.info("Generating Top-X comparison plots...")
        
        results = {}
        
        # Top-X bars for each benchmark
        for benchmark in self.df["benchmark"].unique():
            for metric in ["mae", "rmse"]:
                results[f"topx_{benchmark}_{metric}"] = self.plot_topx_bars(benchmark, metric)
                results[f"topx_stability_{benchmark}_{metric}"] = self.plot_topx_stability(benchmark, metric)
        
        # Cross-problem heatmaps
        for metric in ["mae", "rmse"]:
            results[f"cross_top1_{metric}"] = self.plot_cross_problem_top1_heatmap(metric)
            
            # By sampler
            for sampler in self.df["sampler"].unique():
                results[f"cross_top1_{metric}_{sampler}"] = self.plot_cross_problem_top1_heatmap(
                    metric, sampler=sampler
                )
            
            # By cv_mode
            for cv_mode in self.df["cv_mode"].unique():
                results[f"cross_top1_{metric}_{cv_mode}"] = self.plot_cross_problem_top1_heatmap(
                    metric, cv_mode=cv_mode
                )
        
        plt.close('all')
        logging.info(f"Top-X plots saved to: {self.output_dir}")
        return results


# =============================================================================
# GP HYPERPARAM ATLAS GENERATOR
# =============================================================================

class GPHyperparamAtlasGenerator:
    """
    Generates GP prediction visualizations across hyperparameter configurations.
    
    Note: Requires ability to regenerate datasets from stored parameters (seed, sampler, n_train).
    """
    
    MAX_ATLAS_CONFIGS_DEFAULT = 25
    
    def __init__(self, master_df: pd.DataFrame, output_dir: Path,
                 max_configs: int = None):
        self.df = master_df.copy()
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(parents=True, exist_ok=True)
        self.max_configs = max_configs or self.MAX_ATLAS_CONFIGS_DEFAULT
        
        # Filter to GP models only
        self.gp_df = self.df[self.df["model_family"] == "GP"].copy()
    
    def _select_atlas_configs(self, benchmark: str, sampler: str, cv_mode: str,
                               n_train: int, noise: str,
                               metric: str = "mae") -> pd.DataFrame:
        """
        Select configurations for atlas (controlled to avoid explosion).
        """
        ascending = metric != "r2"
        
        subset = self.gp_df[
            (self.gp_df["benchmark"] == benchmark) &
            (self.gp_df["sampler"] == sampler) &
            (self.gp_df["cv_mode"] == cv_mode) &
            (self.gp_df["n_train"] == n_train) &
            (self.gp_df["noise"] == noise)
        ]
        
        if subset.empty:
            return pd.DataFrame()
        
        # Sort by metric
        subset = subset.sort_values(metric, ascending=ascending)
        
        # Select top configs with diversity (at least one per kernel)
        selected = []
        kernels_seen = set()
        
        for _, row in subset.iterrows():
            kernel = row["model_variant"]
            
            # Always include top by metric
            if len(selected) < self.max_configs // 2:
                selected.append(row)
                kernels_seen.add(kernel)
            # Then ensure diversity
            elif kernel not in kernels_seen and len(selected) < self.max_configs:
                selected.append(row)
                kernels_seen.add(kernel)
            
            if len(selected) >= self.max_configs:
                break
        
        return pd.DataFrame(selected)
    
    def plot_gp_ntrain_sweep(self, benchmark: str, sampler: str,
                              model_variant: str = "GP_Matern52",
                              save: bool = True) -> plt.Figure:
        """
        Show how GP predictions change with n_train.
        
        Note: This is a simplified version showing metrics evolution.
        Full prediction plots would require regenerating datasets.
        """
        subset = self.gp_df[
            (self.gp_df["benchmark"] == benchmark) &
            (self.gp_df["sampler"] == sampler) &
            (self.gp_df["model_variant"] == model_variant)
        ]
        
        if subset.empty:
            return None
        
        n_trains = sorted(subset["n_train"].unique())
        
        fig, axes = plt.subplots(1, 3, figsize=(15, 5))
        
        # MAE vs n_train
        agg = subset.groupby("n_train")["mae"].agg(["mean", "std"]).reset_index()
        axes[0].errorbar(agg["n_train"], agg["mean"], yerr=agg["std"],
                         fmt='o-', capsize=4, color='#e74c3c', linewidth=2)
        axes[0].set_xlabel("n_train")
        axes[0].set_ylabel("MAE")
        axes[0].set_title("MAE Evolution")
        axes[0].grid(True, alpha=0.3)
        
        # Coverage vs n_train
        if "coverage_95" in subset.columns:
            agg = subset.groupby("n_train")["coverage_95"].agg(["mean", "std"]).reset_index()
            axes[1].errorbar(agg["n_train"], agg["mean"], yerr=agg["std"],
                             fmt='o-', capsize=4, color='#3498db', linewidth=2)
            axes[1].axhline(y=0.95, color='green', linestyle='--', label='Target 95%')
            axes[1].set_xlabel("n_train")
            axes[1].set_ylabel("Coverage 95%")
            axes[1].set_title("Coverage Evolution")
            axes[1].legend()
            axes[1].grid(True, alpha=0.3)
        
        # NLPD vs n_train
        if "nlpd" in subset.columns:
            agg = subset.groupby("n_train")["nlpd"].agg(["mean", "std"]).reset_index()
            axes[2].errorbar(agg["n_train"], agg["mean"], yerr=agg["std"],
                             fmt='o-', capsize=4, color='#9b59b6', linewidth=2)
            axes[2].set_xlabel("n_train")
            axes[2].set_ylabel("NLPD")
            axes[2].set_title("NLPD Evolution")
            axes[2].grid(True, alpha=0.3)
        
        fig.suptitle(f"{model_variant} | {benchmark} | {sampler}",
                     fontsize=14, fontweight='bold', y=1.02)
        plt.tight_layout()
        
        if save:
            path = self.output_dir / f"ntrain_sweep_{benchmark}_{sampler}_{model_variant}.png"
            fig.savefig(path, bbox_inches='tight', dpi=150)
            plt.close(fig)
        
        return fig
    
    def plot_sampler_comparison_gp(self, benchmark: str, model_variant: str = "GP_Matern52",
                                    n_train: int = None, save: bool = True) -> plt.Figure:
        """
        Side-by-side comparison of GP performance with Sobol vs LHS.
        """
        subset = self.gp_df[
            (self.gp_df["benchmark"] == benchmark) &
            (self.gp_df["model_variant"] == model_variant)
        ]
        
        if n_train:
            subset = subset[subset["n_train"] == n_train]
        
        if subset.empty:
            return None
        
        fig, axes = plt.subplots(2, 2, figsize=(12, 10))
        
        metrics = ["mae", "coverage_95", "nlpd", "sharpness"]
        titles = ["MAE", "Coverage 95%", "NLPD", "Sharpness"]
        
        for ax, metric, title in zip(axes.ravel(), metrics, titles):
            if metric not in subset.columns:
                ax.set_visible(False)
                continue
            
            # Aggregate by sampler
            agg = subset.groupby("sampler")[metric].agg(["mean", "std"]).reset_index()
            
            colors = [SAMPLER_PALETTE.get(s, "#7f8c8d") for s in agg["sampler"]]
            bars = ax.bar(agg["sampler"], agg["mean"], color=colors, edgecolor='black', alpha=0.8)
            
            if agg["std"].notna().any():
                ax.errorbar(range(len(agg)), agg["mean"], yerr=agg["std"],
                            fmt='none', color='black', capsize=6)
            
            ax.set_ylabel(metric.upper())
            ax.set_title(title, fontweight='bold')
        
        fig.suptitle(f"{model_variant} | {benchmark} | Sobol vs LHS",
                     fontsize=14, fontweight='bold', y=1.02)
        plt.tight_layout()
        
        if save:
            suffix = f"_n{n_train}" if n_train else ""
            path = self.output_dir / f"sampler_comparison_{benchmark}_{model_variant}{suffix}.png"
            fig.savefig(path, bbox_inches='tight', dpi=150)
            plt.close(fig)
        
        return fig
    
    def generate_atlas_index(self) -> Path:
        """Generate markdown index for GP atlas."""
        index_path = self.output_dir / "GP_ATLAS_INDEX.md"
        
        lines = [
            "# GP Hyperparam Atlas Index",
            "",
            f"Max configs per combination: {self.max_configs}",
            "",
            "## n_train Sweep Plots",
            "",
        ]
        
        # List files
        for f in sorted(self.output_dir.glob("ntrain_sweep_*.png")):
            lines.append(f"- [{f.stem}]({f.name})")
        
        lines.extend([
            "",
            "## Sampler Comparison Plots",
            "",
        ])
        
        for f in sorted(self.output_dir.glob("sampler_comparison_*.png")):
            lines.append(f"- [{f.stem}]({f.name})")
        
        with open(index_path, 'w', encoding='utf-8') as f:
            f.write('\n'.join(lines))
        
        return index_path
    
    def generate_all_plots(self) -> Dict[str, Any]:
        """Generate all GP atlas plots."""
        logging.info("Generating GP Hyperparam Atlas plots...")
        
        if self.gp_df.empty:
            logging.warning("No GP data found in results")
            return {}
        
        results = {}
        
        benchmarks = self.gp_df["benchmark"].unique()
        samplers = self.gp_df["sampler"].unique()
        gp_variants = self.gp_df["model_variant"].unique()
        
        # n_train sweep for each combination
        for benchmark in benchmarks:
            for sampler in samplers:
                for variant in gp_variants:
                    key = f"ntrain_sweep_{benchmark}_{sampler}_{variant}"
                    results[key] = self.plot_gp_ntrain_sweep(benchmark, sampler, variant)
        
        # Sampler comparison
        for benchmark in benchmarks:
            for variant in gp_variants:
                key = f"sampler_comp_{benchmark}_{variant}"
                results[key] = self.plot_sampler_comparison_gp(benchmark, variant)
        
        # Generate index
        self.generate_atlas_index()
        
        plt.close('all')
        logging.info(f"GP Atlas plots saved to: {self.output_dir}")
        return results


# =============================================================================
# REPORT INDEX GENERATOR
# =============================================================================

def generate_report_index(output_dir: Path, inventory: BenchmarkInventoryMultiMode) -> Path:
    """Generate main report index markdown."""
    index_path = output_dir / "indices" / "REPORT_INDEX.md"
    index_path.parent.mkdir(parents=True, exist_ok=True)
    
    lines = [
        "# Benchmark Visual Evaluation Report (Multi-Mode)",
        "",
        f"**Session**: {inventory.session_name}",
        f"**Timestamp**: {inventory.timestamp}",
        f"**Total Time**: {inventory.total_time_s:.1f}s",
        "",
        "## Configuration",
        "",
        f"- **Samplers**: {', '.join(inventory.samplers)}",
        f"- **CV Modes**: {', '.join(inventory.cv_modes)}",
        f"- **Training Sizes**: {inventory.n_train_values}",
        f"- **Benchmarks**: {', '.join(inventory.benchmarks)}",
        f"- **Noise Types**: {', '.join(inventory.noise_types)}",
        f"- **Models**: {', '.join(inventory.models)}",
        f"- **Total Results**: {inventory.n_results}",
        "",
        "## Directory Structure",
        "",
        "```",
        "report_multimode/",
        "├── tables/",
        "│   ├── master_table.csv",
        "│   ├── leaderboard_*.csv",
        "│   ├── stability_*.csv",
        "│   └── topX_by_problem/",
        "├── figures/",
        "│   ├── global/           (original global plots)",
        "│   ├── global_extended/  (new multi-mode global plots)",
        "│   ├── sampling_effects/ (sobol vs lhs, n_train effects)",
        "│   ├── cv_diagnostics/   (simple vs nested comparison)",
        "│   ├── comparisons_topX/ (top-X analysis)",
        "│   └── gp_hyperparam_atlas/ (GP-specific analysis)",
        "└── indices/",
        "    ├── REPORT_INDEX.md   (this file)",
        "    ├── GP_ATLAS_INDEX.md",
        "    └── TOPX_INDEX.md",
        "```",
        "",
        "## Navigation",
        "",
        "### Tables",
        "- [Master Table](../tables/master_table.csv)",
        "- [Global Leaderboard MAE](../tables/leaderboard_global_mae.csv)",
        "- [Stability Analysis](../tables/stability_mae.csv)",
        "- [Best Model by Factor](../tables/best_model_per_benchmark_by_sampler_cv_ntrain_mae.csv)",
        "",
        "### Figures",
        "",
        "#### Sampling Effects",
        "- [Sobol vs LHS Comparison](../figures/sampling_effects/sobol_vs_lhs_mae.png)",
        "- [n_train Effect Heatmap](../figures/sampling_effects/ntrain_effect_heatmap_mae.png)",
        "",
        "#### CV Diagnostics",
        "- [Simple vs Nested Distribution](../figures/cv_diagnostics/simple_vs_nested_mae_global.png)",
        "",
        "#### Top-X Comparisons",
        "- [Cross-Problem Top-1 Heatmap](../figures/comparisons_topX/cross_problem_top1_mae_all_all.png)",
        "",
        "#### GP Atlas",
        "- [GP Atlas Index](GP_ATLAS_INDEX.md)",
        "",
    ]
    
    with open(index_path, 'w', encoding='utf-8') as f:
        f.write('\n'.join(lines))
    
    logging.info(f"Generated report index: {index_path}")
    return index_path


# =============================================================================
# MAIN REPORT GENERATOR
# =============================================================================

def generate_benchmark_report_multimode(
    session_name: str,
    results_json_path: Path = None,
    output_dir: Path = None,
    strict: bool = True,
    top_x: int = 10,
    max_atlas_configs: int = 25,
) -> Path:
    """
    Generate complete multi-mode benchmark visual evaluation report.
    
    Args:
        session_name: Name or partial name of benchmark session
        results_json_path: Explicit path to results JSON (auto-detected if None)
        output_dir: Output directory (auto-generated if None)
        strict: If True, abort if requirements (sobol+lhs, simple+nested) not met
        top_x: Number of top configurations to analyze
        max_atlas_configs: Max GP configs for atlas (anti-explosion)
        
    Returns:
        Path to report directory
        
    Raises:
        ValueError: If strict=True and requirements not met
        FileNotFoundError: If results JSON not found
    """
    logging.basicConfig(
        level=logging.INFO,
        format="%(asctime)s | %(levelname)s | %(message)s"
    )
    
    print("=" * 70)
    print(f"BENCHMARK VISUAL EVALUATION (MULTI-MODE): {session_name}")
    print("=" * 70)
    
    # Auto-detect results file
    if results_json_path is None:
        # Search in LOGS_DIR/benchmarks/
        search_dir = LOGS_DIR / "benchmarks"
        candidates = list(search_dir.glob(f"*{session_name}*"))
        
        if not candidates:
            raise FileNotFoundError(f"No session found matching '{session_name}' in {search_dir}")
        
        # Look for comprehensive_results.json
        for candidate in candidates:
            if candidate.is_dir():
                json_path = candidate / "comprehensive_results.json"
                if json_path.exists():
                    results_json_path = json_path
                    break
        
        if results_json_path is None:
            raise FileNotFoundError(f"No comprehensive_results.json found in candidates: {candidates}")
    
    results_json_path = Path(results_json_path)
    logging.info(f"Loading results from: {results_json_path}")
    
    # Load and verify data
    loader = MultiModeResultsLoader(results_json_path, strict=strict)
    
    try:
        requirements = loader.verify_requirements()
        logging.info(f"Requirements verified: samplers={requirements.samplers_found}, cv_modes={requirements.cv_modes_found}")
    except ValueError as e:
        print(f"\n❌ {e}")
        print("\nPara usar benchmark_visual_reporter_multimode, el JSON debe contener:")
        print("  - samplers: sobol Y lhs")
        print("  - cv_modes: simple Y tuning/nested")
        print("\nEncontrado en este JSON:")
        print(f"  - samplers: {requirements.samplers_found}")
        print(f"  - cv_modes: {requirements.cv_modes_found}")
        raise
    
    df = loader.build_master_table()
    inventory = loader.get_inventory()
    
    # Setup output directory
    if output_dir is None:
        output_dir = results_json_path.parent / "report_multimode"
    output_dir = Path(output_dir)
    output_dir.mkdir(parents=True, exist_ok=True)
    
    logging.info(f"Output directory: {output_dir}")
    
    # Create directory structure
    dirs = {
        "tables": output_dir / "tables",
        "global": output_dir / "figures" / "global",
        "global_extended": output_dir / "figures" / "global_extended",
        "sampling_effects": output_dir / "figures" / "sampling_effects",
        "cv_diagnostics": output_dir / "figures" / "cv_diagnostics",
        "comparisons_topX": output_dir / "figures" / "comparisons_topX",
        "gp_atlas": output_dir / "figures" / "gp_hyperparam_atlas",
        "indices": output_dir / "indices",
    }
    
    for d in dirs.values():
        d.mkdir(parents=True, exist_ok=True)
    
    # =========================================================================
    # STEP 1: Export Master Table
    # =========================================================================
    print("\n[1/7] Exporting master table...")
    loader.export_master_table(dirs["tables"])
    
    # =========================================================================
    # STEP 2: Generate Tables
    # =========================================================================
    print("\n[2/7] Generating tables...")
    table_gen = MultiModeTableGenerator(df, dirs["tables"])
    table_gen.generate_all_tables()
    
    # =========================================================================
    # STEP 3: Generate Original Global Plots (reuse existing)
    # =========================================================================
    print("\n[3/7] Generating global plots (original style)...")
    
    # Transform df to match original format expected by BenchmarkGlobalPlotter
    column_mapping = {
        "noise": "noise_type",
        "fit_time": "fit_time_s",
        "predict_time": "predict_time_s",
    }
    df_original = df.rename(columns=column_mapping)
    
    global_plotter = BenchmarkGlobalPlotter(df_original, dirs["global"])
    global_plotter.generate_all_plots()
    
    # =========================================================================
    # STEP 4: Sampling Effects Plots
    # =========================================================================
    print("\n[4/7] Generating sampling effects plots...")
    sampling_plotter = SamplingEffectsPlotter(df, dirs["sampling_effects"])
    sampling_plotter.generate_all_plots()
    
    # =========================================================================
    # STEP 5: CV Diagnostics Plots
    # =========================================================================
    print("\n[5/7] Generating CV diagnostics plots...")
    cv_plotter = CVDiagnosticsPlotter(df, dirs["cv_diagnostics"])
    cv_plotter.generate_all_plots()
    
    # =========================================================================
    # STEP 6: Top-X Comparison Plots
    # =========================================================================
    print("\n[6/7] Generating Top-X comparison plots...")
    topx_comparator = TopXComparator(df, dirs["comparisons_topX"], top_x=top_x)
    topx_comparator.generate_all_plots()
    
    # =========================================================================
    # STEP 7: GP Hyperparam Atlas
    # =========================================================================
    print("\n[7/7] Generating GP Hyperparam Atlas...")
    gp_atlas = GPHyperparamAtlasGenerator(df, dirs["gp_atlas"], max_configs=max_atlas_configs)
    gp_atlas.generate_all_plots()
    
    # =========================================================================
    # Generate Report Index
    # =========================================================================
    print("\nGenerating report index...")
    generate_report_index(output_dir, inventory)
    
    # =========================================================================
    # Summary
    # =========================================================================
    print("\n" + "=" * 70)
    print("REPORT GENERATION COMPLETE")
    print("=" * 70)
    print(f"\n✓ Output directory: {output_dir}")
    print(f"✓ Total records processed: {len(df)}")
    print(f"✓ Samplers: {inventory.samplers}")
    print(f"✓ CV modes: {inventory.cv_modes}")
    print(f"✓ Training sizes: {inventory.n_train_values}")
    print(f"✓ Benchmarks: {len(inventory.benchmarks)}")
    print(f"✓ Models: {len(inventory.models)}")
    print(f"\n📄 Report Index: {output_dir / 'indices' / 'REPORT_INDEX.md'}")
    
    return output_dir


# =============================================================================
# CLI
# =============================================================================

def main():
    """Command line interface."""
    import argparse
    
    parser = argparse.ArgumentParser(
        description="Generate multi-mode benchmark visual evaluation report",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Examples:
    # Auto-detect session by name
    python -m src.evaluation.benchmark_visual_reporter_multimode --session comprehensive_20260121
    
    # Explicit path
    python -m src.evaluation.benchmark_visual_reporter_multimode --json path/to/comprehensive_results.json
    
    # Custom options
    python -m src.evaluation.benchmark_visual_reporter_multimode --session my_session --topx 15 --max_atlas 30
    
    # Non-strict mode (continue even if requirements not fully met)
    python -m src.evaluation.benchmark_visual_reporter_multimode --session partial_run --no-strict
        """
    )
    
    parser.add_argument(
        "--session", "-s",
        type=str,
        default=None,
        help="Session name (or partial match) to process"
    )
    
    parser.add_argument(
        "--json", "-j",
        type=str,
        default=None,
        help="Explicit path to comprehensive_results.json"
    )
    
    parser.add_argument(
        "--output", "-o",
        type=str,
        default=None,
        help="Output directory (default: auto-generated in session folder)"
    )
    
    parser.add_argument(
        "--topx",
        type=int,
        default=10,
        help="Number of top configurations to analyze (default: 10)"
    )
    
    parser.add_argument(
        "--max_atlas_configs",
        type=int,
        default=25,
        help="Max GP configs for atlas to avoid explosion (default: 25)"
    )
    
    parser.add_argument(
        "--no-strict",
        action="store_true",
        help="Continue even if not all requirements (sobol+lhs, simple+nested) are met"
    )
    
    args = parser.parse_args()
    
    if not args.session and not args.json:
        parser.error("Either --session or --json must be provided")
    
    try:
        output_dir = generate_benchmark_report_multimode(
            session_name=args.session or "",
            results_json_path=Path(args.json) if args.json else None,
            output_dir=Path(args.output) if args.output else None,
            strict=not args.no_strict,
            top_x=args.topx,
            max_atlas_configs=args.max_atlas_configs,
        )
        print(f"\n✅ Report generated successfully: {output_dir}")
        
    except ValueError as e:
        print(f"\n❌ Validation Error: {e}")
        return 1
    except FileNotFoundError as e:
        print(f"\n❌ File Not Found: {e}")
        return 1
    except Exception as e:
        print(f"\n❌ Unexpected Error: {e}")
        traceback.print_exc()
        return 1
    
    return 0


if __name__ == "__main__":
    exit(main())


