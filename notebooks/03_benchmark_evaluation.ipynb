{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "30863706",
   "metadata": {},
   "source": [
    "# üß™ Evaluaci√≥n de Modelos Sustitutos en Benchmarks Sint√©ticos\n",
    "\n",
    "Este notebook demuestra c√≥mo usar el m√≥dulo de benchmarks sint√©ticos para evaluar modelos sustitutos (surrogate models). El objetivo es comparar diferentes modelos (GP, Ridge, PLS, etc.) en funciones de test conocidas para entender su comportamiento antes de aplicarlos a datos reales.\n",
    "\n",
    "## Contenido\n",
    "1. **Setup**: Configuraci√≥n inicial\n",
    "2. **Benchmarks**: Exploraci√≥n de funciones de test disponibles\n",
    "3. **Sampling**: Estrategias de muestreo (Sobol, LHS, Grid)\n",
    "4. **Noise**: Tipos de ruido para evaluar robustez\n",
    "5. **Evaluaci√≥n**: Comparaci√≥n de modelos sustitutos\n",
    "6. **M√©tricas**: RMSE, NLPD, cobertura, calibraci√≥n\n",
    "7. **Visualizaci√≥n**: Gr√°ficas de resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bd53a56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup - Imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Configuraci√≥n de plots\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "plt.rcParams['font.size'] = 11\n",
    "\n",
    "# Imports del proyecto\n",
    "import sys\n",
    "sys.path.insert(0, '..')\n",
    "\n",
    "# Benchmarks\n",
    "from src.benchmarks import (\n",
    "    get_benchmark, list_benchmarks,\n",
    "    get_sampler,\n",
    "    get_noise_injector,\n",
    "    generate_benchmark_dataset,\n",
    "    generate_multi_benchmark_suite,\n",
    ")\n",
    "\n",
    "# Models\n",
    "from src.models.gp import GPSurrogateRegressor\n",
    "from src.models.ridge import RidgeSurrogateRegressor\n",
    "from src.models.pls import PLSSurrogateRegressor\n",
    "from src.models.dummy import DummySurrogateRegressor\n",
    "\n",
    "# Metrics & Evaluation\n",
    "from src.analysis.surrogate_metrics import compute_surrogate_metrics\n",
    "from src.analysis.benchmark_runner import (\n",
    "    evaluate_model_on_dataset,\n",
    "    evaluate_models_on_suite,\n",
    "    run_quick_benchmark,\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Setup completo!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a428e4d9",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. üìä Funciones Benchmark Disponibles\n",
    "\n",
    "Tenemos funciones de diferentes dimensionalidades y caracter√≠sticas:\n",
    "\n",
    "| Funci√≥n | Dim | Caracter√≠sticas | Uso |\n",
    "|---------|-----|-----------------|-----|\n",
    "| Forrester | 1D | Una variable, visualizable | Test b√°sico de interpolaci√≥n |\n",
    "| Branin | 2D | Multimodal (3 m√≠nimos) | Test de modelado 2D |\n",
    "| Six-Hump Camel | 2D | Multimodal (6 m√≠nimos locales) | Test de complejidad |\n",
    "| Goldstein-Price | 2D | Muy no lineal | Test de no-linealidad extrema |\n",
    "| Hartmann-3 | 3D | Suave, multimodal | Test de dimensi√≥n media |\n",
    "| Ishigami | 3D | Interacciones, sensitivity | Test de interacciones |\n",
    "| Hartmann-6 | 6D | Est√°ndar en BO | Test de dimensi√≥n |\n",
    "| Borehole | 8D | Modelo f√≠sico | Test realista |\n",
    "| Wing Weight | 10D | Modelo de ingenier√≠a | Test de alta dimensi√≥n |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4720427f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ver todos los benchmarks disponibles\n",
    "print(\"Benchmarks disponibles:\")\n",
    "for info in list_benchmarks(include_info=True):\n",
    "    opt = f\"{info['optimal_value']:.4f}\" if info['optimal_value'] is not None else \"N/A\"\n",
    "    print(f\"  {info['name']:20s} | dim={info['dim']:2d} | √≥ptimo={opt}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47dd1449",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizaci√≥n de Forrester 1D\n",
    "bench = get_benchmark(\"forrester\")\n",
    "\n",
    "# Crear grid denso para visualizaci√≥n\n",
    "X_viz = np.linspace(0, 1, 200).reshape(-1, 1)\n",
    "y_viz = bench(X_viz)\n",
    "\n",
    "# Puntos de muestra (Sobol)\n",
    "sampler = get_sampler(\"sobol\", seed=42)\n",
    "X_sample = sampler.sample_bounds(10, bench.bounds)\n",
    "y_sample = bench(X_sample)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "ax.plot(X_viz, y_viz, 'b-', lw=2, label='Forrester 1D (funci√≥n real)')\n",
    "ax.scatter(X_sample, y_sample, c='red', s=100, zorder=5, label=f'Muestras Sobol (n={len(X_sample)})')\n",
    "ax.axhline(bench.optimal_value, color='green', linestyle='--', alpha=0.5, label=f'√ìptimo global: {bench.optimal_value:.3f}')\n",
    "ax.set_xlabel('x')\n",
    "ax.set_ylabel('f(x)')\n",
    "ax.set_title(f'{bench.name}: Funci√≥n de test t√≠pica en Bayesian Optimization')\n",
    "ax.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e3f5364",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizaci√≥n de Branin 2D\n",
    "bench2d = get_benchmark(\"branin\")\n",
    "\n",
    "# Grid 2D\n",
    "n_grid = 100\n",
    "x1 = np.linspace(bench2d.bounds[0][0], bench2d.bounds[0][1], n_grid)\n",
    "x2 = np.linspace(bench2d.bounds[1][0], bench2d.bounds[1][1], n_grid)\n",
    "X1, X2 = np.meshgrid(x1, x2)\n",
    "X_grid = np.column_stack([X1.ravel(), X2.ravel()])\n",
    "y_grid = bench2d(X_grid).reshape(n_grid, n_grid)\n",
    "\n",
    "# Muestras Sobol\n",
    "X_sample_2d = sampler.sample_bounds(30, bench2d.bounds)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Contour\n",
    "ax = axes[0]\n",
    "cs = ax.contourf(X1, X2, y_grid, levels=30, cmap='viridis')\n",
    "ax.scatter(X_sample_2d[:, 0], X_sample_2d[:, 1], c='red', s=50, edgecolor='white', label='Muestras Sobol')\n",
    "plt.colorbar(cs, ax=ax)\n",
    "ax.set_xlabel('x‚ÇÅ')\n",
    "ax.set_ylabel('x‚ÇÇ')\n",
    "ax.set_title(f'{bench2d.name}: Mapa de contorno')\n",
    "ax.legend()\n",
    "\n",
    "# 3D surface\n",
    "ax = fig.add_subplot(1, 2, 2, projection='3d')\n",
    "ax.plot_surface(X1, X2, y_grid, cmap='viridis', alpha=0.8)\n",
    "ax.set_xlabel('x‚ÇÅ')\n",
    "ax.set_ylabel('x‚ÇÇ')\n",
    "ax.set_zlabel('f(x)')\n",
    "ax.set_title(f'{bench2d.name}: Superficie 3D')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d0fd4eb",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. üéØ Estrategias de Muestreo (DOE)\n",
    "\n",
    "El muestreo afecta mucho la calidad del surrogate. Comparamos:\n",
    "- **Sobol**: Secuencias quasi-aleatorias, excelente cobertura (recomendado)\n",
    "- **LHS**: Latin Hypercube, cl√°sico en ingenier√≠a\n",
    "- **Grid**: Regular, solo para visualizaci√≥n en 1D/2D\n",
    "- **Random**: Baseline, peor cobertura"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "906efa91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparar estrategias de muestreo en 2D\n",
    "fig, axes = plt.subplots(1, 4, figsize=(16, 4))\n",
    "n_samples = 50\n",
    "\n",
    "for ax, name in zip(axes, [\"sobol\", \"lhs\", \"grid\", \"random\"]):\n",
    "    sampler = get_sampler(name, seed=42)\n",
    "    X = sampler.sample(n_samples, dim=2)\n",
    "    \n",
    "    ax.scatter(X[:, 0], X[:, 1], alpha=0.7)\n",
    "    ax.set_xlim(0, 1)\n",
    "    ax.set_ylim(0, 1)\n",
    "    ax.set_aspect('equal')\n",
    "    ax.set_title(f'{name.upper()} (n={len(X)})')\n",
    "    ax.set_xlabel('x‚ÇÅ')\n",
    "    ax.set_ylabel('x‚ÇÇ')\n",
    "\n",
    "plt.suptitle('Comparaci√≥n de Estrategias de Muestreo', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0da16b32",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. üìà Tipos de Ruido\n",
    "\n",
    "Evaluamos robustez ante diferentes tipos de ruido:\n",
    "- **Sin ruido**: Test de interpolaci√≥n pura\n",
    "- **Gaussiano**: Ruido homosced√°stico (varianza constante)\n",
    "- **Heterosced√°stico**: Varianza depende de x (dif√≠cil para GPs est√°ndar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d564d20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demostrar tipos de ruido en Forrester\n",
    "bench = get_benchmark(\"forrester\")\n",
    "X_test = np.linspace(0, 1, 100).reshape(-1, 1)\n",
    "y_clean = bench(X_test)\n",
    "\n",
    "noise_types = [\n",
    "    (\"none\", {}, \"Sin ruido (interpolaci√≥n)\"),\n",
    "    (\"gaussian\", {\"sigma\": 0.3}, \"Gaussiano (œÉ=0.3)\"),\n",
    "    (\"heteroscedastic\", {\"sigma_base\": 0.1, \"sigma_scale\": 0.5}, \"Heterosced√°stico\"),\n",
    "]\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "for ax, (noise_name, noise_kwargs, title) in zip(axes, noise_types):\n",
    "    noise = get_noise_injector(noise_name, seed=42, **noise_kwargs)\n",
    "    y_noisy = noise.add_noise(y_clean.copy(), X_test)\n",
    "    \n",
    "    ax.plot(X_test, y_clean, 'b-', lw=2, label='Funci√≥n real', alpha=0.5)\n",
    "    ax.scatter(X_test, y_noisy, c='red', s=10, alpha=0.5, label='Observaciones')\n",
    "    ax.set_title(title)\n",
    "    ax.set_xlabel('x')\n",
    "    ax.set_ylabel('y')\n",
    "    ax.legend()\n",
    "\n",
    "plt.suptitle('Tipos de Ruido en Datos Sint√©ticos', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03d0b988",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. üî¨ Generaci√≥n de Dataset Completo\n",
    "\n",
    "La funci√≥n `generate_benchmark_dataset` crea un dataset listo para entrenar y evaluar:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98c0e5e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generar dataset para Forrester con ruido gaussiano\n",
    "dataset = generate_benchmark_dataset(\n",
    "    benchmark=\"forrester\",\n",
    "    n_train=30,\n",
    "    n_test=100,\n",
    "    sampler=\"sobol\",\n",
    "    noise=\"gaussian\",\n",
    "    noise_kwargs={\"sigma\": 0.1},\n",
    "    seed=42,\n",
    ")\n",
    "\n",
    "print(f\"Dataset generado: {dataset}\")\n",
    "print(f\"\\nX_train shape: {dataset.X_train.shape}\")\n",
    "print(f\"y_train range: [{dataset.y_train.min():.3f}, {dataset.y_train.max():.3f}]\")\n",
    "print(f\"X_test shape: {dataset.X_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c5557e7",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. ü§ñ Entrenamiento y Evaluaci√≥n de un Modelo\n",
    "\n",
    "Ejemplo completo con GP en Forrester 1D:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84374748",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entrenar GP\n",
    "gp = GPSurrogateRegressor(n_restarts_optimizer=5)\n",
    "gp.fit(dataset.X_train, dataset.y_train)\n",
    "\n",
    "# Predicci√≥n con incertidumbre\n",
    "mean_pred, std_pred = gp.predict_dist(dataset.X_test)\n",
    "\n",
    "# Calcular m√©tricas extendidas\n",
    "metrics = gp.compute_extended_metrics(dataset.y_test_clean, mean_pred, std_pred)\n",
    "\n",
    "print(\"=\" * 50)\n",
    "print(\"M√âTRICAS DEL MODELO GP\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"\\nüìä Precisi√≥n:\")\n",
    "print(f\"   MAE  = {metrics.mae:.4f}\")\n",
    "print(f\"   RMSE = {metrics.rmse:.4f}\")\n",
    "print(f\"   R¬≤   = {metrics.r2:.4f}\")\n",
    "\n",
    "print(f\"\\nüéØ Incertidumbre:\")\n",
    "print(f\"   NLPD (menor mejor)    = {metrics.nlpd:.4f}\")\n",
    "print(f\"   Coverage 95%          = {metrics.coverage_95:.2%} (ideal: 95%)\")\n",
    "print(f\"   Error de calibraci√≥n  = {metrics.calibration_error_95:.4f}\")\n",
    "print(f\"   Ancho medio IC 95%    = {metrics.mean_interval_width_95:.4f}\")\n",
    "print(f\"   Sharpness (mean œÉ)    = {metrics.sharpness:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0e95fc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizaci√≥n del ajuste GP en 1D\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "# Ordenar para plotting\n",
    "idx = np.argsort(dataset.X_test.ravel())\n",
    "X_sorted = dataset.X_test[idx]\n",
    "mean_sorted = mean_pred[idx]\n",
    "std_sorted = std_pred[idx]\n",
    "y_true_sorted = dataset.y_test_clean[idx]\n",
    "\n",
    "# Funci√≥n real\n",
    "ax.plot(X_sorted, y_true_sorted, 'k-', lw=2, label='Funci√≥n real', zorder=3)\n",
    "\n",
    "# Predicci√≥n GP\n",
    "ax.plot(X_sorted, mean_sorted, 'b-', lw=2, label='Predicci√≥n GP', zorder=4)\n",
    "\n",
    "# Bandas de confianza\n",
    "ax.fill_between(X_sorted.ravel(), \n",
    "                mean_sorted - 1.96*std_sorted, \n",
    "                mean_sorted + 1.96*std_sorted,\n",
    "                alpha=0.2, color='blue', label='95% CI')\n",
    "ax.fill_between(X_sorted.ravel(), \n",
    "                mean_sorted - std_sorted, \n",
    "                mean_sorted + std_sorted,\n",
    "                alpha=0.3, color='blue', label='68% CI')\n",
    "\n",
    "# Puntos de entrenamiento\n",
    "ax.scatter(dataset.X_train, dataset.y_train, c='red', s=80, \n",
    "           edgecolor='white', zorder=5, label=f'Train (n={dataset.n_train})')\n",
    "\n",
    "ax.set_xlabel('x', fontsize=12)\n",
    "ax.set_ylabel('y', fontsize=12)\n",
    "ax.set_title(f'GP en {dataset.benchmark_name} | RMSE={metrics.rmse:.4f}, R¬≤={metrics.r2:.3f}, Coverage={metrics.coverage_95:.1%}')\n",
    "ax.legend(loc='upper right')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fc249ed",
   "metadata": {},
   "source": [
    "---\n",
    "## 6. ‚öîÔ∏è Comparaci√≥n de M√∫ltiples Modelos\n",
    "\n",
    "Comparamos GP, Ridge, PLS y Dummy en el mismo dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea2b0c97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definir modelos a comparar\n",
    "models = {\n",
    "    \"Dummy\": DummySurrogateRegressor(),\n",
    "    \"Ridge\": RidgeSurrogateRegressor(),\n",
    "    \"PLS (2 comp)\": PLSSurrogateRegressor(n_components=1),  # 1 para 1D\n",
    "    \"GP (Mat√©rn 3/2)\": GPSurrogateRegressor(n_restarts_optimizer=3),\n",
    "}\n",
    "\n",
    "# Evaluar cada modelo\n",
    "results = {}\n",
    "for name, model in models.items():\n",
    "    model.fit(dataset.X_train, dataset.y_train)\n",
    "    mean, std = model.predict_dist(dataset.X_test)\n",
    "    metrics = model.compute_extended_metrics(dataset.y_test_clean, mean, std)\n",
    "    results[name] = {\n",
    "        \"mean\": mean,\n",
    "        \"std\": std,\n",
    "        \"metrics\": metrics\n",
    "    }\n",
    "\n",
    "# Tabla comparativa\n",
    "comparison_data = []\n",
    "for name, r in results.items():\n",
    "    m = r[\"metrics\"]\n",
    "    comparison_data.append({\n",
    "        \"Model\": name,\n",
    "        \"RMSE\": m.rmse,\n",
    "        \"MAE\": m.mae,\n",
    "        \"R¬≤\": m.r2,\n",
    "        \"NLPD\": m.nlpd if m.nlpd else \"N/A\",\n",
    "        \"Coverage 95%\": f\"{m.coverage_95:.1%}\" if m.coverage_95 else \"N/A\",\n",
    "    })\n",
    "\n",
    "comparison_df = pd.DataFrame(comparison_data)\n",
    "print(\"\\nüìä COMPARACI√ìN DE MODELOS\")\n",
    "print(\"=\"*70)\n",
    "print(comparison_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6381a39c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizaci√≥n comparativa\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "axes = axes.ravel()\n",
    "\n",
    "colors = ['red', 'green', 'orange', 'blue']\n",
    "\n",
    "for ax, (name, r), color in zip(axes, results.items(), colors):\n",
    "    idx = np.argsort(dataset.X_test.ravel())\n",
    "    X_sorted = dataset.X_test[idx]\n",
    "    mean_sorted = r[\"mean\"][idx]\n",
    "    y_true_sorted = dataset.y_test_clean[idx]\n",
    "    \n",
    "    # Funci√≥n real\n",
    "    ax.plot(X_sorted, y_true_sorted, 'k-', lw=2, alpha=0.5, label='Funci√≥n real')\n",
    "    \n",
    "    # Predicci√≥n\n",
    "    ax.plot(X_sorted, mean_sorted, color=color, lw=2, label='Predicci√≥n')\n",
    "    \n",
    "    # Bandas si hay std\n",
    "    if r[\"std\"] is not None:\n",
    "        std_sorted = r[\"std\"][idx]\n",
    "        ax.fill_between(X_sorted.ravel(), \n",
    "                        mean_sorted - 1.96*std_sorted, \n",
    "                        mean_sorted + 1.96*std_sorted,\n",
    "                        alpha=0.2, color=color)\n",
    "    \n",
    "    # Training points\n",
    "    ax.scatter(dataset.X_train, dataset.y_train, c='gray', s=40, alpha=0.5)\n",
    "    \n",
    "    m = r[\"metrics\"]\n",
    "    ax.set_title(f'{name}\\nRMSE={m.rmse:.4f}, R¬≤={m.r2:.3f}')\n",
    "    ax.legend(loc='upper right', fontsize=9)\n",
    "\n",
    "plt.suptitle(f'Comparaci√≥n de Modelos en {dataset.benchmark_name}', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e695282",
   "metadata": {},
   "source": [
    "---\n",
    "## 7. üöÄ Evaluaci√≥n en M√∫ltiples Benchmarks\n",
    "\n",
    "Usando `run_quick_benchmark` para una evaluaci√≥n r√°pida:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbe63962",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluaci√≥n r√°pida en varios benchmarks\n",
    "models_to_test = {\n",
    "    \"Dummy\": DummySurrogateRegressor(),\n",
    "    \"Ridge\": RidgeSurrogateRegressor(),\n",
    "    \"PLS\": PLSSurrogateRegressor(n_components=2),\n",
    "    \"GP\": GPSurrogateRegressor(n_restarts_optimizer=3),\n",
    "}\n",
    "\n",
    "suite_results = run_quick_benchmark(\n",
    "    models=models_to_test,\n",
    "    benchmarks=[\"forrester\", \"branin\", \"hartmann3\"],\n",
    "    n_train=50,\n",
    "    n_test=200,\n",
    "    noise_sigma=0.1,\n",
    "    seed=42,\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f14ca35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ver tabla de resultados\n",
    "summary_df = suite_results.get_summary_df()\n",
    "print(\"\\nüìä RESUMEN DE RESULTADOS\")\n",
    "print(summary_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cec3cde3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gr√°fico de barras por modelo\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# RMSE por benchmark\n",
    "ax = axes[0]\n",
    "pivot_rmse = summary_df.pivot(index='benchmark', columns='model', values='rmse')\n",
    "pivot_rmse.plot(kind='bar', ax=ax, width=0.8)\n",
    "ax.set_title('RMSE por Benchmark (menor es mejor)')\n",
    "ax.set_ylabel('RMSE')\n",
    "ax.legend(title='Modelo')\n",
    "ax.tick_params(axis='x', rotation=45)\n",
    "\n",
    "# R¬≤ por benchmark\n",
    "ax = axes[1]\n",
    "pivot_r2 = summary_df.pivot(index='benchmark', columns='model', values='r2')\n",
    "pivot_r2.plot(kind='bar', ax=ax, width=0.8)\n",
    "ax.set_title('R¬≤ por Benchmark (mayor es mejor)')\n",
    "ax.set_ylabel('R¬≤')\n",
    "ax.legend(title='Modelo')\n",
    "ax.tick_params(axis='x', rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8127507",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ranking final de modelos\n",
    "print(\"\\nüèÜ RANKING FINAL (por RMSE promedio)\")\n",
    "print(\"=\"*50)\n",
    "print(suite_results.get_model_ranking(\"rmse\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0fed87e",
   "metadata": {},
   "source": [
    "---\n",
    "## 8. üìù Resumen y Conclusiones\n",
    "\n",
    "### Uso t√≠pico:\n",
    "\n",
    "```python\n",
    "# 1. Generar dataset\n",
    "dataset = generate_benchmark_dataset(\n",
    "    benchmark=\"branin\",  # Elegir funci√≥n de test\n",
    "    n_train=50,          # Muestras de entrenamiento\n",
    "    n_test=200,          # Muestras de test\n",
    "    sampler=\"sobol\",     # Estrategia de muestreo\n",
    "    noise=\"gaussian\",    # Tipo de ruido\n",
    "    noise_kwargs={\"sigma\": 0.1},\n",
    "    seed=42,\n",
    ")\n",
    "\n",
    "# 2. Entrenar modelo\n",
    "model = GPSurrogateRegressor()\n",
    "model.fit(dataset.X_train, dataset.y_train)\n",
    "\n",
    "# 3. Predecir con incertidumbre\n",
    "mean, std = model.predict_dist(dataset.X_test)\n",
    "\n",
    "# 4. Evaluar m√©tricas\n",
    "metrics = model.compute_extended_metrics(dataset.y_test_clean, mean, std)\n",
    "print(f\"RMSE: {metrics.rmse:.4f}\")\n",
    "print(f\"NLPD: {metrics.nlpd:.4f}\")\n",
    "print(f\"Coverage 95%: {metrics.coverage_95:.2%}\")\n",
    "```\n",
    "\n",
    "### Para comparar modelos masivamente:\n",
    "\n",
    "```python\n",
    "from src.analysis.benchmark_runner import run_quick_benchmark\n",
    "\n",
    "results = run_quick_benchmark(\n",
    "    models={\"GP\": GPSurrogateRegressor(), \"Ridge\": RidgeSurrogateRegressor()},\n",
    "    benchmarks=[\"forrester\", \"branin\", \"hartmann3\", \"hartmann6\"],\n",
    ")\n",
    "print(results.get_model_ranking(\"rmse\"))\n",
    "```\n",
    "\n",
    "### M√©tricas importantes:\n",
    "- **RMSE/MAE**: Precisi√≥n de predicci√≥n\n",
    "- **R¬≤**: Varianza explicada\n",
    "- **NLPD**: Calidad de incertidumbre (penaliza sobre/sub-confianza)\n",
    "- **Coverage 95%**: ¬øEl 95% de puntos cae en el IC 95%?\n",
    "- **Calibration Error**: |Coverage - 0.95|"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
